{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09638601-2c9f-4a5c-b19d-73c34d4e058f",
   "metadata": {},
   "source": [
    "# Árboles De Decisión: Regresión Y Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfdbb3c-7aea-47cc-96a2-595466cd8c8b",
   "metadata": {},
   "source": [
    "Los árboles de decisión son modelos predictivos formados por reglas binarias (si/no) con las que se consigue repartir las observaciones en función de sus atributos y predecir así el valor de la variable respuesta.  \n",
    "  \n",
    "Muchos métodos predictivos generan modelos globales en los que una única ecuación se aplica a todo el espacio muestral. Cuando el caso de uso implica múltiples predictores, que interaccionan entre ellos de forma compleja y no lineal, es muy difícil encontrar un único modelo global que sea capaz de reflejar la relación entre las variables. Los métodos estadísticos y de machine learning basados en árboles engloban a un conjunto de técnicas supervisadas no paramétricas que consiguen segmentar el espacio de los predictores en regiones simples, dentro de las cuales es más sencillo manejar las interacciones. Es esta característica la que les proporciona gran parte de su potencial.  \n",
    "  \n",
    "  \n",
    "**VENTAJAS**  \n",
    "* Los árboles son fáciles de interpretar aun cuando las relaciones entre predictores son complejas.  \n",
    "* Los modelos basados en un solo árbol (no es el caso de random forest, boosting) se pueden representar gráficamente aun cuando el número de predictores es mayor de 3.  \n",
    "* Los árboles pueden, en teoría, manejar tanto predictores numéricos como categóricos sin tener que crear variables dummy o one-hot-encoding. En la práctica, esto depende de la implementación del algoritmo que tenga cada librería.  \n",
    "* Al tratarse de métodos no paramétricos, no es necesario que se cumpla ningún tipo de distribución específica.  \n",
    "* Por lo general, requieren mucha menos limpieza y preprocesado de los datos en comparación con otros métodos de aprendizaje estadístico (por ejemplo, no requieren estandarización).  \n",
    "* No se ven muy influenciados por outliers.  \n",
    "* Si para alguna observación, el valor de un predictor no está disponible, a pesar de no poder llegar a ningún nodo terminal, se puede conseguir una predicción empleando todas las observaciones que pertenecen al último nodo alcanzado. La precisión de la predicción se verá reducida pero al menos podrá obtenerse.  \n",
    "* Son muy útiles en la exploración de datos, permiten identificar de forma rápida y eficiente las variables (predictores) más importantes.  \n",
    "* Son capaces de seleccionar predictores de forma automática.  \n",
    "* Pueden aplicarse a problemas de regresión y clasificación.  \n",
    "  \n",
    "\n",
    "**DESVENTAJAS**  \n",
    "* La capacidad predictiva de los modelos basados en un único árbol es bastante inferior a la conseguida con otros modelos. Esto es debido a su tendencia al overfitting y alta varianza. Sin embargo, existen técnicas más complejas que, haciendo uso de la combinación de múltiples árboles (bagging, random forest, boosting), consiguen mejorar en gran medida este problema.  \n",
    "* Son sensibles a datos de entrenamiento desbalanceados (una de las clases domina sobre las demás).  \n",
    "* Cuando tratan con predictores continuos, pierden parte de su información al categorizarlos en el momento de la división de los nodos.  \n",
    "* Tal y como se describe más adelante, la creación de las ramificaciones de los árboles se consigue mediante el algoritmo de recursive binary splitting. Este algoritmo identifica y evalúa las posibles divisiones de cada predictor acorde a una determinada medida (RSS, Gini, entropía…). Los predictores continuos tienen mayor probabilidad de contener, solo por azar, algún punto de corte óptimo, por lo que suelen verse favorecidos en la creación de los árboles.  \n",
    "* No son capaces de extrapolar fuera del rango de los predictores observado en los datos de entrenamiento.  \n",
    "  \n",
    "  \n",
    "## Arboles de decisión en Python  \n",
    "  \n",
    "La principal implementación de árboles de decisión en Python está disponible en la librería scikit-learn a través de las clases DecisionTreeClassifier y DecisionTreeRegressor. Una característica importante para aquellos que han utilizado otras implementaciones es que, en scikit-learn, es necesario convertir las variables categóricas en variables dummy (one-hot-encoding).  \n",
    "  \n",
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Entorno01",
   "language": "python",
   "name": "entorno01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
